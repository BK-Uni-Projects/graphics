<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title></title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="reveal.js/css/reveal.min.css"/>
    <style type="text/css">code{white-space: pre;}</style>
    <link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>
    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">


<section><section id="pipeline-overview" class="titleslide slide level1"><h1>Pipeline Overview</h1></section><section id="putting-the-fun-in-fundamentals" class="slide level2">
<h1>Putting the <strong>&quot;Fun&quot;</strong> in Fundamentals</h1>
</section></section>
<section><section id="section" class="titleslide slide level1"><h1></h1></section><section id="vertices-in-world-space" class="slide level2">
<h1>Vertices in World Space</h1>
<p>The pipeline is fed (<em>somehow, we'll cover later</em>) with <strong>vertices</strong> in <strong>WORLD SPACE</strong> (or <strong>Object Space</strong>)</p>
<p><img src="assets/pipelineOverview/pipelineOverview01verticesInWorldSpace.png" alt="pipelineOverview01verticesInWorldSpace" /> </p>
</section></section>
<section><section id="section-1" class="titleslide slide level1"><h1></h1></section><section id="clip-space-transformation" class="slide level2">
<h1>Clip Space Transformation</h1>
<ul>
<li>Transform the vertices into a certain region of space
<ul>
<li>in OpenGL this is called <strong>clip space</strong></li>
</ul></li>
<li>Also known as <strong>projection transformation</strong>
<ul>
<li><a href="http://arcsynthesis.org/gltut/Basics/Intro%20Graphics%20and%20Rendering.html">http://arcsynthesis.org/gltut/Basics/Intro%20Graphics%20and%20Rendering.html</a></li>
<li>you get to <strong>program</strong> this bit (GLSL - vertex shader)</li>
</ul></li>
</ul>
<p><img src="assets/pipelineOverview/worldSpaceToClipSpace.png" alt="pipelineOverview01verticesInWorldSpace" /> </p>
</section></section>
<section><section id="section-2" class="titleslide slide level1"><h1></h1></section><section id="homogeneous-coordinates" class="slide level2">
<h1>Homogeneous Coordinates!!</h1>
<ul>
<li>X,Y,Z and W!!!
<ul>
<li>W defines what the extents of clip space are for this vertex (-W to W on X, Y, Z)</li>
</ul></li>
<li>The process of transforming vertices into clip space is arbitrary (stupid)
<ul>
<li>OpenGL provides a lot of flexibility in this step</li>
<li>i.e. <strong>programmable</strong> flexibility!</li>
</ul></li>
<li>We'll cover homogeneous coordinates more later
<ul>
<li>http://www.tomdalling.com/blog/modern-opengl/explaining-homogenous-coordinates-and-projective-geometry/</li>
<li>http://www.songho.ca/math/homogeneous/homogeneous.html</li>
</ul></li>
</ul>
</section></section>
<section><section id="section-3" class="titleslide slide level1"><h1></h1></section><section id="vertices-in-clip-space" class="slide level2">
<h1>Vertices in Clip Space</h1>
<p><img src="assets/pipelineOverview/pipelineOverview02verticesInClipSpace.png" alt="pipelineOverview02verticesInClipSpace" /> </p>
</section></section>
<section><section id="section-4" class="titleslide slide level1"><h1></h1></section><section id="normalized-device-coordinates" class="slide level2">
<h1>Normalized Device Coordinates</h1>
<ul>
<li>Clip space is interesting, but inconvenient
<ul>
<li>the extent of this space is different for each vertex, which makes visualizing a triangle rather difficult</li>
</ul></li>
<li>The X, Y, and Z of each vertex's position is divided by W to get normalized device coordinates
<ul>
<li>basically the same as clip space except that the range of X, Y and Z are <strong>[-1, 1]</strong></li>
</ul></li>
</ul>
</section></section>
<section><section id="section-5" class="titleslide slide level1"><h1></h1></section><section id="vertices-in-normalized-device-coordinates" class="slide level2">
<h1>Vertices in Normalized Device Coordinates</h1>
<p><img src="assets/pipelineOverview/pipelineOverview03verticesInNormalizedDeviceCoordinates.png" alt="pipelineOverview03verticesInNormalizedDeviceCoordinates" /> </p>
</section></section>
<section><section id="section-6" class="titleslide slide level1"><h1></h1></section><section id="clipping" class="slide level2">
<h1>Clipping</h1>
<ul>
<li>Triangles not <strong>fully</strong> in <strong>clip space</strong> (the [-1,1] cube) we want to <strong>clip</strong></li>
</ul>
<p><img src="assets/pipelineOverview/clipping.png" alt="pipelineOverview02verticesInClipSpace" /> </p>
</section></section>
<section><section id="section-7" class="titleslide slide level1"><h1></h1></section><section id="clipping-2" class="slide level2">
<h1>Clipping 2</h1>
<ul>
<li>Points are easy to test in our new, normalised coordinate space (NDC)</li>
<li>Lines are more difficult</li>
<li>Triangles are complicated
<ul>
<li>may need to make more triangles</li>
</ul></li>
</ul>
</section></section>
<section><section id="section-8" class="titleslide slide level1"><h1></h1></section><section id="vertices-in-clipped-ndc" class="slide level2">
<h1>Vertices in Clipped NDC</h1>
<p><img src="assets/pipelineOverview/pipelineOverview04verticesInNormalizedDeviceCoordinatesClipped.png" alt="pipelineOverview04verticesInNormalizedDeviceCoordinatesClipped" /> </p>
</section></section>
<section><section id="section-9" class="titleslide slide level1"><h1></h1></section><section id="window-transformation" class="slide level2">
<h1>Window Transformation</h1>
<ul>
<li>Transform from normalized device coordinates towindow coordinates</li>
<li>window coordinates are relative to the window that OpenGL is running within</li>
<li>Though they refer to the window, they are still three dimensional coordinates
<ul>
<li>still floating-point values</li>
<li>still have a z-coordinates!! Why?? Stay-tuned ...</li>
<li>bottom-left position is the origin (0, 0)</li>
</ul></li>
</ul>
</section></section>
<section><section id="section-10" class="titleslide slide level1"><h1></h1></section><section id="vertices-in-window-coordinates" class="slide level2">
<h1>Vertices in Window Coordinates</h1>
<p><img src="assets/pipelineOverview/pipelineOverview05verticesInWindowCoordinates.png" alt="pipelineOverview05verticesInWindowCoordinates" /> </p>
</section></section>
<section><section id="section-11" class="titleslide slide level1"><h1></h1></section><section id="scan-conversion" class="slide level2">
<h1>Scan Conversion</h1>
<ul>
<li>After conversion to window coordinates the triangle undergoes a process called <strong>scan conversion</strong>
<ul>
<li>also known as <strong>rasterization</strong></li>
</ul></li>
<li>Takes the triangle and generates fragments that cover the area of the triangle
<ul>
<li>also fills in pixels for lines</li>
</ul></li>
<li>We'll look later at how scan conversion can be done</li>
</ul>
</section></section>
<section><section id="section-12" class="titleslide slide level1"><h1></h1></section><section id="scan-conversion-triangles-again" class="slide level2">
<h1>Scan Conversion (triangles again)</h1>
<ul>
<li>Image in the centre shows the digital grid of output pixels
<ul>
<li>the circles represent the centre of each pixel, the centre of each pixel represents a <strong>sample</strong>:
<ul>
<li>a discrete location within the area of a pixel</li>
</ul></li>
</ul></li>
</ul>
<p><img src="assets/pipelineOverview/scanline.png" alt="pipelineOverview02verticesInClipSpace" /> </p>
</section></section>
<section><section id="section-13" class="titleslide slide level1"><h1></h1></section><section id="scan-conversion-more-triangles" class="slide level2">
<h1>Scan Conversion (more triangles)</h1>
<ul>
<li>During scan conversion, a triangle will produce a <strong>fragment</strong> for every pixel sample that is within the 2D area of the triangle (right)
<ul>
<li>a rough <strong>approximation</strong> of the triangle's general shape</li>
</ul></li>
</ul>
<p><img src="assets/pipelineOverview/scanline.png" alt="pipelineOverview02verticesInClipSpace" /> </p>
</section></section>
<section><section id="section-14" class="titleslide slide level1"><h1></h1></section><section id="scan-conversion-1" class="slide level2">
<h1>Scan Conversion</h1>
<h3 id="shared-edges-and-the-invariance-guarantee-by-tom-clancy">Shared Edges and <strong><em>the invariance guarantee</em></strong> (By Tom Clancy)</h3>
<ul>
<li>triangles that share edges are very often rendered (unless you’re making asteroids)</li>
<li>OpenGL offers the invariance guarantee
<ul>
<li>so long as shared edge vertex positions are <strong>identical</strong>, there will be no sample gaps during scan conversion</li>
</ul></li>
<li>What would happen otherwise??</li>
</ul>
<p><img src="assets/pipelineOverview/sharededge.png" alt="pipelineOverview02verticesInClipSpace" /> </p>
</section></section>
<section><section id="section-15" class="titleslide slide level1"><h1></h1></section><section id="scan-conversion-still" class="slide level2">
<h1>Scan Conversion (Still??)</h1>
<ul>
<li>Scan conversion only uses X and Y position of the triangle in window coordinates to determine which fragments to generate
<ul>
<li>the Z value is not forgotten!</li>
</ul></li>
<li>The result of scan converting a triangle is a sequence of fragments that cover the shape of the triangle
<ul>
<li>each fragment has data associated with it
<ul>
<li>the 2D location of the fragment in window coordinates</li>
<li>the Z position of the fragment</li>
<li>(a.k.a. known as the depth of the fragment)</li>
<li>there may be other information that is part of a fragment</li>
</ul></li>
</ul></li>
</ul>
</section></section>
<section><section id="section-16" class="titleslide slide level1"><h1></h1></section><section id="fragments" class="slide level2">
<h1>Fragments</h1>
<p><img src="assets/pipelineOverview/pipelineOverview06fragments.png" alt="pipelineOverview06fragments" /> </p>
</section></section>
<section><section id="section-17" class="titleslide slide level1"><h1></h1></section><section id="fragment-processing" class="slide level2">
<h1>Fragment Processing</h1>
<ul>
<li>Fragment processing takes a fragment from scan converted triangle and <strong>transforms</strong> it into</li>
<li>one or more colour values</li>
<li>a single depth value (z value)</li>
<li>you get to <strong>program</strong> this bit (GLSL - fragment shader)</li>
<li>The order that fragments from a single triangle are processed in is irrelevant
<ul>
<li><em>unlike</em> the order that triangles are pumped into the pipeline, a single triangle lies in a <strong>single plane</strong> so fragments generated from it <strong>cannot overlap</strong></li>
</ul></li>
<li><strong>However</strong>, the fragments from another triangle might</li>
<li><strong><em>Therefore, fragments from one triangle must all be processed before fragments from another triangle</em></strong></li>
</ul>
</section></section>
<section><section id="section-18" class="titleslide slide level1"><h1></h1></section><section id="processed-fragments" class="slide level2">
<h1>Processed Fragments</h1>
<p><img src="assets/pipelineOverview/pipelineOverview07fragmentsProcessed.png" alt="pipelineOverview07fragmentsProcessed" /> </p>
</section></section>
<section><section id="section-19" class="titleslide slide level1"><h1></h1></section><section id="fragment-writing-maybe" class="slide level2">
<h1>Fragment Writing (maybe)</h1>
<ul>
<li>After generating one or more colours and a depth value
<ul>
<li>the fragment is <strong>possibly</strong> written to the destination (probably a frame buffer)</li>
<li><strong>What might control this?</strong></li>
</ul></li>
<li>This step involves more than simply writing to the destination image
<ul>
<li>These will be covered more later</li>
</ul></li>
</ul>
</section></section>
<section><section id="section-20" class="titleslide slide level1"><h1></h1></section><section id="colours" class="slide level2">
<h1>Colours</h1>
<ul>
<li>The usual description of a colour is as a series of numbers on the range <strong>[0, 1] </strong><em>Why [0,1]?</em>
<ul>
<li>each of the numbers corresponds to the <strong>intensity</strong> of a particular reference colour</li>
<li>the final colour represented by the series of numbers is a mix of these reference colours</li>
</ul></li>
<li>The set of reference colours is called a <strong>colour space</strong>.
<ul>
<li>the most common colour space for screens is RGB, where the reference colours are Red, Green and Blue</li>
<li>printed works tend to use CMYK (Cyan, Magenta, Yellow, Black)]</li>
</ul></li>
</ul>
<p><img src="assets/pipelineOverview/RGB_and_CMYK_comparison.png" alt="pipelineOverview07fragmentsProcessed" /> </p>
</section></section>
<section><section id="section-21" class="titleslide slide level1"><h1></h1></section><section id="colours-2" class="slide level2">
<h1>Colours 2</h1>
<ul>
<li>Combining different intensities of this 3 colours, we can generate and display millions of different colour shades in OpenGL
<ul>
<li>how many colours? what is the usual representation?
<ul>
<li><a href="http://en.wikipedia.org/wiki/Color_depth#Deep_color_.2830.2F36.2F48-bit.29" class="uri">http://en.wikipedia.org/wiki/Color_depth#Deep_color_.2830.2F36.2F48-bit.29</a></li>
</ul></li>
<li>in most conditions more than the human eye can perceive
<ul>
<li>any <em>special conditions?</em></li>
</ul></li>
<li>unless you're a tetrachromat ... <a href="http://www.post-gazette.com/pg/06256/721190-114.stm" class="uri">http://www.post-gazette.com/pg/06256/721190-114.stm</a></li>
</ul></li>
</ul>
</section></section>
<section><section id="section-22" class="titleslide slide level1"><h1></h1></section><section id="shaders" class="slide level2">
<h1>Shaders</h1>
<ul>
<li>A shader is a program designed to be run on a renderer as part of the rendering operation</li>
<li>Regardless of the kind of rendering system in use, shaders can only be executed at certain points in the rendering process</li>
<li>These shader stages represent hooks where we can insert our code to create specific visual effects, for example:
<ul>
<li>transformation of an incoming vertex to clip space is a useful hook for user-defined code</li>
<li>the processing of a fragment into final colours and depth</li>
</ul></li>
</ul>
</section></section>
<section><section id="section-23" class="titleslide slide level1"><h1></h1></section><section id="shaders-2-glsl" class="slide level2">
<h1>Shaders 2 (GLSL)</h1>
<ul>
<li>Shaders for OpenGL are run on the actual rendering hardware so free up valuable CPU time for other tasks
<ul>
<li>and <strong>usually</strong> the graphics card has orders of magnitude more raw, though specialist processing power available</li>
<li>GeForce GTX 760 Ti - <strong>2460 GFLOPS</strong> single-precision!!</li>
</ul></li>
<li>Or perform operations that would be difficult if not impossible without the flexibility of executing arbitrary code
<ul>
<li>however, they live within certain limits that CPU code would not have to ...</li>
<li>?? any guess what limits ??</li>
</ul></li>
<li>There are a number of shading languages available to various APIs. The one used here is the primary shading language of OpenGL, the <strong>OpenGL Shading Language</strong>, or <strong>GLSL</strong>. for short
<ul>
<li>It looks deceptively like C, <strong>but it is very much not C</strong></li>
</ul></li>
</ul>
</section></section>
<section><section id="section-24" class="titleslide slide level1"><h1></h1></section><section id="shaders-3-glsl" class="slide level2">
<h1>Shaders 3 (GLSL)</h1>
<pre class="glsl"><code>#version 330

layout(location = 0) in vec4 position;
uniform float loopDuration;
uniform float time;

void main()
{
    float timeScale = 3.14159f * 2.0f / loopDuration;
    
    float currTime = mod(time, loopDuration);
    vec4 totalOffset = vec4(
        cos(currTime * timeScale) * 0.5f,
        sin(currTime * timeScale) * 0.5f,
        0.0f,
        0.0f);

    gl_Position = position + totalOffset;
}</code></pre>
</section></section>
    </div>
  </div>


  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.min.js"></script>

  <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        theme: 'sky', // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
//          { src: 'reveal.js/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; }, }
//          { src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
]});
    </script>
    </body>
</html>
